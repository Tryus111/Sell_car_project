{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17sym3q0MFLVEPIe1CPwo03R5YIKOancc",
      "authorship_tag": "ABX9TyOTohx7GZs1ImrO8ZQQG5JL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tryus111/Sell_car_project/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smcO_T-0i22Q"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Jun 25 13:31:46 2023\n",
        "\n",
        "@author: user\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#import file\n",
        "df_cleaned = pd.read_csv('/train.csv')\n",
        "# Preprocessing for deep learning\n",
        "df_raw = df_cleaned.copy()\n",
        "\n",
        "# Clean Unknown Data\n",
        "df_raw = df_raw[df_raw['engine_size'] != 0]\n",
        "df_raw = df_raw[df_raw['fuel_type'] != 'Unknown']\n",
        "df_raw = df_raw[df_raw['drivetrain'] !='Unknown']\n",
        "\n",
        "# Price\n",
        "df_raw['price'] = pd.to_numeric(df_raw['price'], errors='coerce')\n",
        "df_raw['price'] = df_raw['price'].astype(float)\n",
        "\n",
        "# Change inf value to valid value\n",
        "df_raw = df_raw.replace([np.inf, -np.inf], np.nan)\n",
        "df_raw = df_raw.dropna()\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "# One Hot Encoding\n",
        "index_column = [0, 1, 4, 6, 8, 9, 33, 34]\n",
        "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(),index_column)], remainder = 'passthrough')\n",
        "df_raw = ct.fit_transform(df_raw)\n",
        "\n",
        "if isinstance(df_raw, csr_matrix):\n",
        "    df_raw = df_raw.toarray()\n",
        "\n",
        "df_preprocessing = pd.DataFrame(df_raw, columns=ct.get_feature_names_out())\n",
        "\n",
        "#Standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "unscaled_input = df_preprocessing.iloc[:, : -1]\n",
        "targets = df_preprocessing.iloc[:, -1]\n",
        "targets\n",
        "unscaled_input_2d = unscaled_input.values.reshape(-1, 1)\n",
        "\n",
        "scaled_input = sc.fit_transform(unscaled_input_2d)\n",
        "\n",
        "# Shuffle data\n",
        "shuffle_data = np.arange(scaled_input.shape[0])\n",
        "np.random.shuffle(shuffle_data)\n",
        "\n",
        "shuffle_inputs = scaled_input[shuffle_data]\n",
        "shuffle_targets = targets[shuffle_data]\n",
        "\n",
        "sample_count = shuffle_inputs.shape[0]\n",
        "\n",
        "# Split data\n",
        "training_sample = int(0.8 * sample_count)\n",
        "validation_sample = int(0.1 * sample_count)\n",
        "test_sample = sample_count - training_sample - validation_sample\n",
        "\n",
        "train_inputs = shuffle_inputs[: training_sample]\n",
        "train_targets = shuffle_targets[: training_sample]\n",
        "\n",
        "validation_inputs = shuffle_inputs[training_sample : training_sample + validation_sample]\n",
        "validation_targets = shuffle_targets[training_sample: training_sample + validation_sample]\n",
        "\n",
        "test_inputs = shuffle_inputs[training_sample:]\n",
        "test_targets = shuffle_targets[training_sample:]\n",
        "\n",
        "# Save to npz\n",
        "np.savez('training_data.npz', inputs = train_inputs, targets = train_targets)\n",
        "np.savez('validation_data.npz', inputs = validation_inputs, targets = validation_targets)\n",
        "np.savez('test_data.npz', inputs = test_inputs, targets = test_targets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian Baru"
      ],
      "metadata": {
        "id": "qOff4e7yr4tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian Baru"
      ],
      "metadata": {
        "id": "LGYh366WsRXU"
      }
    }
  ]
}